{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cc23bb-7ef6-4129-9fd3-c397e3c3838d",
   "metadata": {},
   "source": [
    "### Hackathon 3: Optimisation of a Bioprocess with Multifidelity Bayesian Optimisation\n",
    "\n",
    "\n",
    "#### Hackathon Breif\n",
    "This hackathon involves the optimisation of a simulated bioprocess at process scale involving CHO cells to produce a desired protein. (ie. growing and feeding cells under precise conditions to produce the desired product).\n",
    "\n",
    "#### Inputs and Outputs\n",
    "Inputs to the bioprocess includes 5 vairables: the temperature [°C], pH and the concentration of feed [mM] at 3 different timepoints over 150 minutes. The output is the concentration of the titre (desired product) [g/L]. The goal is to obtain the input variables that correspond to the highest obtained titre. \n",
    "\n",
    "The bounds of the inputs are as follows: \n",
    "\n",
    "```\n",
    "temperature [°C]               -> 30 - 40\n",
    "pH                             -> 6 - 8\n",
    "first feed concentration [mM]  -> 0 - 50\n",
    "second feed concentration [mM] -> 0 - 50\n",
    "third feed concentration [mM]  -> 0 - 50\n",
    "```\n",
    "\n",
    "#### Fidelities and Running the simulation\n",
    "The simulations can be perfomed at 3 levels of fidelities with an associated accuracy and costs. These fidelities corresponds to a different reactor type and scale used. \n",
    "\n",
    "```\n",
    "Lowest fideility: 3L reactor with 1 feeding timepoint at 60 mins.\n",
    "Realtive cost: 10\n",
    "Remarks: The feeding concentration is taken as the second feed concentration. Lowest accuracy, but also lowest cost. \n",
    "\n",
    "Middle fidelity: 3L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
    "Relative cost: 575\n",
    "Remarks: -\n",
    "\n",
    "Highest fidelity: 15L reactor with 3 feeding timepoints at 40, 80, 120 mins.\n",
    "Relative cost: 2100\n",
    "Remarks: Highest accuracy but high cost.\n",
    "```\n",
    "\n",
    "To run an experiment, one can use the `vl.conduct_experiment(X)` function -> this is your objective function. The inputs to this function is a matrix of shape (N, 6) where N is the number of data points and 6 refers to the total number of variables in the following order: `[temperature, pH, feed1, feed2, feed3, fidelity]`. The fidelities are refered to as integers where `0` corresponds to the lowest fidelity, `1` with the middle and `2` with the highest fidelity. An example is shown below. \n",
    "\n",
    "``` python\n",
    "def obj_func(X):\n",
    "\treturn (-np.array(vl.conduct_experiment(X)) #negative placed if optimisation performed is minimisation\n",
    "\n",
    "X_initial = np.array([[33, 6.25, 10, 20, 20, 0],\n",
    "                      [38, 8, 20, 10, 20, 0]])\n",
    "Y_initial = vl.conduct_experiment(X_initial)\n",
    "print(Y_initial)\n",
    "```\n",
    "\n",
    "#### Goal and Submission\n",
    "Your goal is to develop a Bayesian Optimisation class to obtain the set of inputs which maximizes the titre. You have a budget of 10000 (observe the cost of running each fidelity), a maximum runtime (on the intructor's computer - be aware of how large the search space becomes especially with 6 dimensions!) and starting with a maximum of 6 training points. (Remember, you have to have at least 2 points for each variable for the covariance matrix to be calculated.)\n",
    "\n",
    "Like in previous hackathons, please submit your BO class (and GP class) along with the execution block to the Stremlit app. A different cell type (with different simulation parameters and maxima) will be used for scoring.\n",
    "\n",
    "This hackathon will be scored based on the sum of the normalised maximum titre concentration obtained.\n",
    "\n",
    "You must stay within the allocated budget! This will be checked, and if exceeded, your submission will be disqualified!\n",
    "\n",
    "#### Form of the BO class and execution block\n",
    "You are allowed to write your own BO class or make modifications to any of the previously seen BO classes. \n",
    "\n",
    "You must include the attributes `self.X` and `self.Y` corresponding to all of your evaluated inputs and outputs as this will be used to retrive the information used for scoring. \n",
    "\n",
    "```python\n",
    "#submission should look something like the following\n",
    "class GP: #if you have any separate classes other than the BO class\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "#BO class\n",
    "class BO: \n",
    "    def __init__(self, ...):\n",
    "        self.X = #training data which the evaluated data is to be appended\n",
    "        self.Y = #evaluated via the objective function using self.X\n",
    "\n",
    "# BO Execution Block\n",
    "X_training = [...]\n",
    "X_seachspace = [...]\n",
    "\n",
    "BO_m = BO(...)\n",
    "```\n",
    "\n",
    "#### Guidance (Advanced)\n",
    "You must develop a multifidelity Bayesian Optimisation algorithm. Your scoring will be additionally penalised by the code runtime in the units of seconds. Make your algorithm as fast as it can go!\n",
    "\n",
    "\n",
    "#### Guidance (Intermediate) \n",
    "It is not mandatory for you to develop a multifidelity BO algorithm. You could, if you choose, use the single or batch BO algorithm developed previously to perform the optimisation. The lowest fidelity experiments do not offer accurate outcomes and you have to choose how many number of expeirments for each fidelity to be performed such that you do not exceed your allocated budget. \n",
    "\n",
    "However, if you do wish to tackle the hackathon via a multifidelity BO by modifying the single batch BO code from the first hackathon. Here are some pointers. \n",
    "1. Observe the output of the `vl.conduct_experiment(X)` function. The output does not have the same array structure/shape as the outputs obtained in the previous sections. You have to modify this in order to accomodate for the BO algorithm.\n",
    "2. Create a new acquisition function that is cost aware. We have previously used Lower Confidence Bound to balance exploration and exploitation of the search space. To make this cost aware, we can scale the values obtained from LCB by the cost.\n",
    "\n",
    "```python\n",
    "    def MF_lower_confidence_bound(...):\n",
    "        lower_std = Ysearchspace_mean - acquisition_hyperparam[0]*np.sqrt(Ysearchspace_std)\n",
    "        # mf_lower_std = lower_std / assocated cost for each simulation\n",
    "        return (X_searchspace[np.argmin(mf_lower_std)])\n",
    "```\n",
    "\n",
    "If this is done succefully, well done! However, you might see that the code will run rather slowly for each iteration (remember how the runtime scales with respect to additional dimensions in the search space). If you are finding it difficult to run the full iterations, a recommendation is to lower the number of total points in your search space. For example, if you are using the np.linspace() function, start with a very course number of points for each dimension (ex. 3) to develop your code. Once you are happy that the code can run without errors, then you can increase the number of points per dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd73484-4185-4126-8d42-fd3b5258cc61",
   "metadata": {},
   "source": [
    "#### Package Imports\n",
    "\n",
    "Packages are limited to the the ones listed in the package cell - Talk to one of the intructors to ask if it is possible to import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc60248-a788-4874-9922-36c336a69fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using google collab, run the following pip installs!\n",
    "!pip install sobol_seq\n",
    "!pip install plotly\n",
    "!pip install gpytorch\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ab0af-7637-40a7-ab13-e7c3491d0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "import plotly.graph_objs as go\n",
    "from scipy.integrate import quad\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import minimize, differential_evolution, NonlinearConstraint\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import time\n",
    "import sobol_seq\n",
    "import torch\n",
    "import gpytorch\n",
    "import copy\n",
    "\n",
    "import virtual_lab as vl\n",
    "import conditions_data as data\n",
    "from utils import standardize_data, unstandardize_y, train_gp_model, \\\n",
    "    expected_improvement, summed_feeding, optimize_acquisition_function, plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae15efe-9687-418d-9b4d-32031d09ae78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
